{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GorohovskiMax/MNIST-handwritten-dataset---ML-implementations/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3jKUkm--pse"
      },
      "source": [
        "# üß†**Handwritten Digit Classification - A Practical Exploration with the MNIST dataset** üîç\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YWvEm2sbRFX"
      },
      "source": [
        "# Introduction:\n",
        "In this assignment, we will analyze and solve the classic **digit classification problem** using the well-known **MNIST Dataset**. The MNIST dataset consists of 70,000 images of **handwritten digits (digits 0 to 9)** that are written in different forms and variations (varying in handwriting). This makes it an excellent benchmark for understanding and implementing machine learning techniques.\n",
        "\n",
        "In this notebook, we will **compare and contrast three Machine Learning approaches** to solve this digit classification problem. We will explore the way in which these approaches work, try to analyze their strengths and weaknesses and analyze which of the approaches can be more suitable for the following problem.\n",
        "\n",
        " In order to do so, the notebook will be organized into **three key parts** as outlined below:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- üîπ**Part A: Multi-Class Perceptron** ü§ñ\n",
        "\n",
        " - In this section of the notebook we will implement the **Perceptron Learning Algorithm** to classify digits. As we will later see, The Perceptron is a **linear binary classifier** and to extend it to multi-class classification problem, we will adopt the **One vs All (OvA)** approach.\n",
        "\n",
        " - This part will involve the use of the **\"Pocket Algorithm\"**, which is an enhancement of the **Perceptron algorithm** for a **linearly inseperable** data.\n",
        "\n",
        "\n",
        "- üîπ**Part B: Softmax Regression** üåê\n",
        "\n",
        " - In this section, we will introduce the **Softmax Regression**, a generalization of **Logistic Regression** for multi-class classification.\n",
        " Rather than producing a single positive or negative output, this algorithm provides the **class probabilities**. This characteristic makes it particularly suitable for problems involving more than two classes (multi-class problems) or for cases where we need a more detailed, probabilistic understanding of how likely each class will be.\n",
        "\n",
        " - We use **Cross-Entropy loss** function for optimization. We will explore the strengths of probabilistic models in handling multi-class classification tasks.\n",
        "\n",
        "\n",
        "- üîπ**Part C: Linear Regression** üìä\n",
        "\n",
        " - In this section, we will explore an approach called **Linear Regression** for classification.\n",
        "\n",
        " - Although **Linear Regression** is being usually used in regression tasks (prediction of a certain continuous variable that is also a dependent variable), we will see that this approach can also be adapted for classification problem, by interpreting outputs and applying suitable threshold\n",
        "\n",
        "---\n",
        "\n",
        "üìö This notebook will provide a clear and comprehensive exploration of the theoretical understanding as well as the practical implementation of the models. We will discuss the results and provide comprehensive analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Unky8IGLjy"
      },
      "source": [
        "###‚è¨Loading The Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6w_xeRgiiOp"
      },
      "source": [
        "To start, we will load the **MNIST dataset** from the **sklearn.datasets**. We will also import all the necessary libraries that we will use for our assignment:\n",
        "1. **sklearn.model_selection** - To use the `train_test_split` function which allows us to separate the dataset into **training data** set and **test data** set\n",
        "\n",
        "2. **seaborn** - To use **attractive** and **informative** statistical graphics\n",
        "\n",
        "3. **matplotlib.pyplot** - To use animated visualizations and graphs. We would use it mainly for the purpose of **confusion matrices** as well as for other informative illustrations such as the **train loss** and **test loss** functions\n",
        "\n",
        "4. **numpy** - for vectoral mathematical computation\n",
        "\n",
        "5. **tqdm.notebook** - illustration of a progress bar. it is used to indicate a progress within the 'for' loops that we will use\n",
        "\n",
        "6. **sklearn.metrics** - used for the confusion matrix display\n",
        "\n",
        "After loading the dataset, we then access **two types of components**:\n",
        "\n",
        " **X**: which would represent the **features**, in our case the **pixels** of the images. Each image consists of 28 pixels in height and 28 pixels in width - resulting in a total of 784 pixels. Each pixel has an intensity value that ranges between 0 (black) to 255 (white).\n",
        "\n",
        " **Y**: which would represent the **labels**, in our case the **digits**. There are 10 digits, ranging between 0 to 9.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uTZzpi0o1WD"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix as confusion_matrix_fn\n",
        "\n",
        "# Fetch MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version = 1)\n",
        "\n",
        "# Access features (pixel values) and labels\n",
        "X, y = mnist['data'], mnist['target']\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SYGzk_bGjzy"
      },
      "source": [
        "### ‚úÇData Splitting:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XPK--xkzHn6"
      },
      "source": [
        "By obtaining the X and y components, we would be able to split the dataset into **two subsets** as mentioned in the assignment:\n",
        "\n",
        "- the first set will contain **60,000 images** and it will be our **training set**.\n",
        "\n",
        "- The second set will contain **10,000 images** and it will be the **test set**.\n",
        "\n",
        "To ensure that the data we will split will always stay the same each time we will run the code, we will specify the **random_state** to be 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CNqQpJJzHJK"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 60000, test_size = 10000, random_state = 42)\n",
        "print(f\"Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFUx-7h7CZyu"
      },
      "source": [
        "In order for us to better evaluate our models, and in order for us to make **accurate predictions and assumptions**, we want to work with a training set that is well distributed. In this way, we will ensure that that the model learns from all classes equally and is not biased towards any specific class.\n",
        "\n",
        "To showcase that, we will make a graph that will visualize the distribution of the digits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MrIGzGNW_Jk"
      },
      "outputs": [],
      "source": [
        "# Assuming 'y_train' contains the labels for the training data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=y_train)\n",
        "plt.xlabel('Digit Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Digits in Training Data')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsMjZTXmCr0_"
      },
      "source": [
        "As we can tell by the graph, the distribution of digits within the training set is well-distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfRrnrFFr-eV"
      },
      "source": [
        "### üí°Data Preparation and Preprocessing for Training and Testing\n",
        "Now when we have our training set and test set ready, we will preprocess them for the **training and testing processes**.\n",
        "\n",
        "The data we currently have consists of categorical variables. In order to make the models perform well on that data, we need to turn that data into numerical data.\n",
        "\n",
        "In addition, we will create **One-Hot-Vector Representation** of the digit labels (for the **y_train** and **y_test**) by **indexing** the identity matrix with those labels. We will also flatten the input images to create a **vector representation of size 785 x 1**.\n",
        "\n",
        "For the **Multi-class Perceptron** algorithm, we will adjust the first component of the matrices to be the bias terms and it will be set to 1, thus creating **X_train_with_bias** and **X_test_with_bias**.\n",
        "\n",
        "For **Softmax Regression** and **Linear Regression**, it would also be essential to scale **X_train** and **X_test**. We do so because in those algorithms, scaling will prevent numerical instabilities when dealing with large numbers (in the case of Softmax Regression, for instance). It will also ensure that there will be equal contribution of features to the learning process, without having dominance by large-scale features (essential for Linear Regression).\n",
        "\n",
        "After the scaling process, we will concatenate the bias component into **X_train_scaled** and **X_test_scaled** to get **X_train_with_bias** and **X_test_with_bias**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOJoTriN2wpu"
      },
      "outputs": [],
      "source": [
        "# Create One-Hot-Vector using Identity Matrix\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "y_onehot_train = np.eye(10)[y_train]\n",
        "y_onehot_test = np.eye(10)[y_test]\n",
        "\n",
        "print(f\" One-hot-vector of y_train: {y_onehot_train.shape}\")\n",
        "print(f\" One-hot-vector of y_test: {y_onehot_test.shape}\")\n",
        "\n",
        "# Add bias to inputs\n",
        "bias = np.ones((X_train.shape[0], 1))\n",
        "X_train_with_bias = np.concatenate((bias, X_train), axis=1)\n",
        "bias = np.ones((X_test.shape[0], 1))\n",
        "X_test_with_bias = np.concatenate((bias, X_test), axis=1)\n",
        "\n",
        "print(f\" X_train for Multi-class Perceptron: {X_train_with_bias.shape}\")\n",
        "print(f\" X_test for Multi-class Perceptron: {X_test_with_bias.shape}\")\n",
        "\n",
        "# Used for the Softmax Regression and Linear Regression\n",
        "X_train_scaled = X_train / 255.0\n",
        "X_test_scaled = X_test / 255.0\n",
        "\n",
        "# Add bias to scaled inputs\n",
        "bias_scaled = np.ones((X_train_scaled.shape[0], 1))\n",
        "X_train_scaled_with_bias = np.concatenate((bias_scaled, X_train_scaled), axis=1)\n",
        "bias_scaled = np.ones((X_test_scaled.shape[0], 1))\n",
        "X_test_scaled_with_bias = np.concatenate((bias_scaled, X_test_scaled), axis=1)\n",
        "\n",
        "print(f\" Scaled X_train: {X_train_scaled_with_bias.shape}\")\n",
        "print(f\" Scaled X_test: {X_test_scaled_with_bias.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW-Y8FOkiFDQ"
      },
      "source": [
        "## Part A: Pocket Perceptron Algorithmü§ñ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDZSnzPDiOvt"
      },
      "source": [
        "In this part we will implement the **Multi-Class Perceptron Algorithm**. It relies on the foundational machine learning technique called the **\"Perceptron Algorithm\"** which is used for binary classification tasks. This algorithm creates a linear decision boundary to separate data into two classes: **(+1) Positive** and **(-1) Negative**.\n",
        "\n",
        "By that, it assumes that the data is **linearly separable** - thus guaranteeing that the algorithm will converge. However, in the case of the MNIST digit dataset, the dataset isn't necessarily linearly separable, which in turn, can cause the algorithm to perform poorly. In that case we will implement the **Pocket Perceptron**.\n",
        "\n",
        "The **Pocket Algorithm** will enhance the stability and the performance of the Perceptron by maintaining, or keeping the best weight vector encounter up to iteration (or epoch) 'i' during the learning process. In order for us to train the model to recognize each digit, we must use a strategy called **One vs. All**.\n",
        "\n",
        "The strategy will involve training **multiple** binary Perceptron classifiers to distinguish one digit from all the other digits. It will allow us to break down a complex multi-class problem into a set of binary problems, effectively training a set of classifiers to handle each class individually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7AgZcAOzQbf"
      },
      "source": [
        "### Pocket Algorithm:\n",
        "We will first implement the **Pocket Perceptron algorithm**. The major enhancement to the algorithm is that now, instead of linearly searching the misclassified examples, we will only generate random samples when we know there are misclassifications. This way, we will improve the performance of the algorithm, without using additional 'for' loop (which in turn, would've highly increased the time complexity of our model). We also keep the training errors and test errors for the results graph that we will showcase later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgEMA-3Z0McU"
      },
      "outputs": [],
      "source": [
        "def pocket_perceptron(X, y, X_test, y_test, max_epochs=100):\n",
        "  num_samples, num_features = X.shape # extract the number of samples with which we work and define the features (x0,...,xn values)\n",
        "  weights = np.zeros(num_features) # hold the weights in a vector of the size of the features (initialized to zeros -> w0=0,...,wn=0)\n",
        "  best_weights = weights.copy() # pocket the best weights we've seen so far (in terms of achieving minimized error)\n",
        "  pocket_error = np.inf # pocket the minimal error\n",
        "  train_loss = [] # for visualization of train loss function\n",
        "  test_loss = [] # for visualization of test loss function\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs), desc='Epoch'):\n",
        "    predictions = np.sign(X @ weights)\n",
        "    misclassified_samples = np.where(predictions!= y)[0] # detect all misclassified samples\n",
        "\n",
        "    if misclassified_samples.size == 0: # no misclassifications detected\n",
        "       break\n",
        "\n",
        "    random_idx = np.random.choice(misclassified_samples) # select random misclassified sample\n",
        "    weights+= y[random_idx] * X[random_idx]  # update w(t) + y(t)x(t) -> w(t+1)\n",
        "    current_error = np.mean(np.sign(X @ weights)!= y) # represents the E-in\n",
        "    test_error = np.mean(np.sign(X_test @ best_weights)!= y_test) # represents the E-out\n",
        "\n",
        "    if current_error < pocket_error: # check whether we have a better error rate, if we do, update the weights and error rate\n",
        "        pocket_error = current_error\n",
        "        best_weights = weights.copy()\n",
        "\n",
        "    # collect train loss and test loss results\n",
        "    train_loss.append(pocket_error)\n",
        "    test_loss.append(test_error)\n",
        "\n",
        "  return best_weights, train_loss, test_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC3AhzU4A2PW"
      },
      "source": [
        "Having the **Pocket Perceptron** algorithm ready, we can implement our `digit_training_perceptron`. This function will employ the **one vs. all** strategy.\n",
        "\n",
        "The idea is straightforward, within each iteration, we will get a new digit and for each such digit (ranging from 0 to 9), we will assign the label (+1) to examples labeled 'i' and (-1) to examples labeled '0','1','2',...,'9' (**excluding 'i'**).\n",
        "\n",
        " In addition, we also maintain **train loss** and **test loss** histories in order for us to later on implement them in the train loss vs. test loss results graph.\n",
        "\n",
        "  The end product that we would get are the resulting weight vectors after training (one for each binary classifier) as well as the training errors and the test errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6efOfGZqAa0J"
      },
      "outputs": [],
      "source": [
        "def digit_train_perceptron(X_train, y_train, X_test, y_test, max_epochs=100):\n",
        "  digits = np.unique(y_train) # Set of digits\n",
        "  weights = [] # Weights for each digit classifier\n",
        "  train_loss_history = [] # for visualization of train loss function\n",
        "  test_loss_history = [] # for visualization of test loss function\n",
        "\n",
        "  # Train the model on all digits and collect the weights and the errors\n",
        "  progress_bar = tqdm(digits, desc='Processing digit 0', total=len(digits))\n",
        "  for digit in progress_bar:\n",
        "    progress_bar.set_description(f'Processing digit {digit}')\n",
        "    binary_labels_train = np.where(y_train == digit, 1, -1) # labels to check for training\n",
        "    binary_labels_test = np.where(y_test == digit, 1, -1) # labels to check for test\n",
        "\n",
        "    best_weight, train_loss, test_loss = (pocket_perceptron(X_train, binary_labels_train, X_test, binary_labels_test, max_epochs))\n",
        "    weights.append(best_weight)\n",
        "    train_loss_history.append(train_loss)\n",
        "    test_loss_history.append(test_loss)\n",
        "\n",
        "\n",
        "  print(\"Training complete\")\n",
        "  return weights, train_loss_history, test_loss_history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dptjIqND9tE"
      },
      "source": [
        "When we will have the trained weights, we would be able to predict the labels for the test samples and get the **predicted labels** from the model. The way in which we do this is by building \"confidence scores\", that represent the activation values for each class. The model then assigns the predicted label for a sample based on the class with the highest confidence score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u66hXMocEghN"
      },
      "outputs": [],
      "source": [
        "def predict_labels(X, weights):\n",
        "  weights = np.array(weights)\n",
        "  confidence_scores = X @ weights.T #Each row represents the confidence score for a sample in all 10 classes of digits\n",
        "  predicted_labels = np.argmax(confidence_scores, axis = 1)\n",
        "  print(\"Predictions made\")\n",
        "  return predicted_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHmo4Uu1JjLW"
      },
      "source": [
        "### Training and Prediction\n",
        "Now when we have provided all the necessary implementations, we are ready to train our model on the **X_train_with_bias** set and the **y_train** set. We also input **X_test_with_bias** and **y_test** into the function in order to get the train loss and test loss results.\n",
        "\n",
        "We have chosen to train the model on max_epochs of 200 because we are working with a big and complex and data, therefore we need to ensure that the model would be able to learn for enough epochs to have opportunities to adjust weights in cases of errors.\n",
        "\n",
        "The end product would be a new list of weights (named **'W'**), **train_loss_history** and **test_loss_history**.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIEoP8tJJ8n2"
      },
      "outputs": [],
      "source": [
        "W, train_loss_history, test_loss_history = digit_train_perceptron(X_train_with_bias, y_train, X_test_with_bias, y_test, max_epochs = 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm9EaTqUYUTe"
      },
      "source": [
        "We have trained our model and we are ready the get the model's predictions. We will make the predictions on the **X_test_with_bias** using the weights we got from the training. The output will be the predictions of the model on the test set that we have made (called **y_pred**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTR4awU5w1Dm"
      },
      "outputs": [],
      "source": [
        "y_pred = predict_labels(X_test_with_bias, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzb8HGS_YZm1"
      },
      "source": [
        "## Results and Graphs:\n",
        "After implementing the **Multi-Class Perceptron algorithm**, we will compute the **confusion matrix** for the multi-class classification problem on the **test data** (using what the machine tried to predict, hence, **y_pred** and the real results, **y_test**).\n",
        "\n",
        "This matrix will describe the performance of our classification model, by  showcasing the amount of times each digit was classified, as well as how it was classified. Ideally, we would want all of the classifications to be set on the **main diagonal** - which would mean that the model classified the digits correctly. But we will see that there will be some misclassifications.\n",
        "\n",
        "In addition, We will also display the accuracy of the model, defined as 'ACC' and it will be calculated by the formula:\n",
        "\n",
        "**ACC = (TP + TN) / (TP + TN + FP + FN)**\n",
        "\n",
        "After doing so, we will also display the sensitivity of each digit. That would help us to better understand where the model struggles to achieve success in classifying digits and where it succeeds to do so.\n",
        "\n",
        "Finally, we will display the training loss function and the test loss function as functions of **epochs**. These important functions will display the process of learning, providing us a visual illustration of how the model processed the data and where eventually did the model achieve it's lowest misclassification rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix:"
      ],
      "metadata": {
        "id": "LhCv6Y-BFcWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx8fMNcKfy5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate Confusion Matrix\n",
        "confusion_matrix = confusion_matrix_fn(y_test, y_pred)\n",
        "classes = confusion_matrix.shape[0]\n",
        "sensitivity = {}\n",
        "binary_confusion_matrix = {}\n",
        "\n",
        "# Accuracy and sensitivity calculation based on the definision in the Maman\n",
        "for i in tqdm(range(classes), desc=\"Generating model's Performance\"):\n",
        "  TP = confusion_matrix[i, i]\n",
        "  FP = np.sum(confusion_matrix[i, :]) - TP\n",
        "  FN = np.sum(confusion_matrix[:, i]) - TP\n",
        "  TN = np.sum(confusion_matrix) - TP - FP - FN\n",
        "  accuracy = (TP + TN) / (TP + TN + FP + FN) # Formula given in the assignment\n",
        "  sensitivity[i] = TP / (TP + FN) # Formula given in the assignment\n",
        "  binary_confusion_matrix[i] = np.array([[TP, FP], [FN, TN]]) # For the visualization of confusion matrices for individual digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZu9CthWZrmS"
      },
      "outputs": [],
      "source": [
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "print(f\"ACC: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fXVQvPyaoNW"
      },
      "source": [
        "We will also provide the **confusion matrix** for each individual digit, and calculate the sensitivity (defined to be TPR) for each of the digit classes. The TPR is calculated as follows:\n",
        "\n",
        "**TPR = TP / (TP + FN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH04RlxJjUI4"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(10), desc='Calculating Confusion Matrices and Sensitivities'):\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = binary_confusion_matrix[i], display_labels = [i,'Other digits'])\n",
        "  cm_display.plot()\n",
        "  plt.show()\n",
        "  print(f\"\\nTPR of digit {i}: {sensitivity[i]}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train loss vs. Test loss graph"
      ],
      "metadata": {
        "id": "76dVV_S_GYuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the **train_loss_history** and **test_loss_history** are lists that contain loss values for training and testing across multiple epochs, we take the average of these values in order to display the train function and test function on the graph."
      ],
      "metadata": {
        "id": "-7vUBLLuGn2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_loss_history = np.mean(np.array(train_loss_history), axis=0)\n",
        "avg_test_loss_history = np.mean(np.array(test_loss_history), axis=0)"
      ],
      "metadata": {
        "id": "w97NxnbgRQpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss function vs. Test loss function per epochs:"
      ],
      "metadata": {
        "id": "L_L6Co-EHfGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7f1Z4k5DYuu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(avg_train_loss_history, label='Training Loss')\n",
        "plt.plot(avg_test_loss_history, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.title('Multi-Class Perceptron: Training vs. Test Loss per epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Evaluation:\n",
        "- Using the multi-class Perceptron approach, we have achieved an accuracy of **0.9635** on the given test set (96.35% accuracy).  \n",
        "\n",
        "- Digit sensitivities are ranging from **0.785** to **0.943** (digit 0 having the highest sensitivity and digit 3 and 4 having the lowest sensitivities)\n",
        "\n",
        "- Train loss and test loss functions decreased as the model learned (more epochs), indicating that it's learning and generalizing well.\n",
        "\n",
        "Using these metrics we can see that the overall performance of the multi-class perceptron is good. The accuracy of the model reflects strong performance.\n",
        "Digit 0 appears to be the most distinguishable likely due to its distinct shape and minimal overlap with other digits. However, digit 3 and digit 4 became the most challenging digits to distinguish, as they tend to share visual similarities with other digits.\n",
        "\n",
        "Moreover, the model falsely indicated that the digit 5 is 3 (97 times) and the digit 5 is 8 (77 times). This also indicates that the model struggles to distinguish between these pairs of digits.\n",
        "\n",
        "In terms of the learning behavior, we expected the training and test losses to start relatively high (since the model's weights are initially random and the model hasn't learned anything yet) and as the model trains, both the training and the test loss functions should decrease and eventually converge. We may see that the model respects this expected outcome and provides a suitable behavior for a well-trained multi-class perceptron.\n",
        "\n",
        "**Strengths**:\n",
        "1. **High Accuracy**\n",
        "\n",
        "2. **Simplicity** - The perceptron is computationally efficient and straightforward to implement\n",
        "\n",
        "**Weaknesses**:\n",
        "1. Difficulty with complex digits - the model tends to struggle with digits that share some structural patterns like 3 or 4.\n",
        "\n",
        "2. Because of the Perceptron Algorithm simple approach, the accuracy might still be considered lower than other more complex models.\n",
        "\n"
      ],
      "metadata": {
        "id": "seuKIyq6Hz91"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FPrcv-Oa_Mv"
      },
      "source": [
        "## Part B: Softmax Regression üåê\n",
        "In this section we will use another technique called **Softmax Regression**. It is an extension of the binary **Logistic Regression** model that deals with multi-class classification task.\n",
        "\n",
        "In binary classification, **Logistic Regression** uses a **sigmoid function** to output the probability that a given input belongs to the positive class (+1) or the negative class (0). However, when dealing with multiple classes, the sigmoid function cannot handle more than two possible outcomes.\n",
        "\n",
        "Therefore, we will use **Softmax Regression** in order to implement the **Softmax function**, which generalizes the sigmoid function to a multi-class problem. The Softmax function computes the probabilities for each class, making sure that the sum of all probabilities is 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "FKSjqTCDVL3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first compute the **Softmax probabilities** for the multi-class classification problem. These probabilities are central to Softmax Regression because they represent the predicted probabilities of each class for a given input.\n",
        "\n",
        "We have the input samples X and the weight matrix W and we first make matrix multiplication to compute the **logits Z** which are essentialy the raw scores for each class and each sample.\n",
        "\n",
        "Since the Softmax function involves exponentials, directly computing `np.exp(Z)` can potentially lead to numerical overflow if Z will contain very large values. Therefore, we subtract the maximum Z value along each row (corresponding to each sample). In this way, we ensure numerical stability without affecting the final probabilities (as the maximum value cancels out in the normalization step).\n",
        "\n",
        "After stabilizing Z, we calculate the **Softmax probabilities** by normalizing the logits using **Softmax function**."
      ],
      "metadata": {
        "id": "3Xc8tnl1WHD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF0URmJRedvZ"
      },
      "outputs": [],
      "source": [
        "def softmax_prob_calc(X,W):\n",
        "  Z = X @ W.T # We compute the logits Z\n",
        "  exp_Z = np.exp(Z - np.max(Z, axis = 1, keepdims=True)) # Avoid overflow for value computation, subtract max value for each row in Z\n",
        "  softmax_probs = exp_Z / np.sum(exp_Z, axis = 1, keepdims=True) # Normalize the logits using softmax function\n",
        "  return softmax_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then calculate the **Cross-entropy loss** using the mean cross-entropy loss formula with the true class labels (**true_y**) and predicted probabilities (**pred_y**).\n",
        "\n",
        "Cross-entropy loss evaluates how well the predicted probability distribution aligns with the true class labels. This loss function will penalize incorrect predictions more heavily when the predicted probability for the correct class is low, encouraging the model to produce higher confidence for correct classifications.\n",
        "\n"
      ],
      "metadata": {
        "id": "WWHQ72yPfzer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MkVLHsi-jK-"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(true_y, pred_y):\n",
        "  loss = -np.mean(np.sum(true_y * np.log(pred_y), axis=1)) # Implementation of mean cross entropy loss\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the functions we have created to implement the **Softmax Regression** model. This implementation trains the model and minimizes the error using **gradient descent method**, an optimization algorithm that reduces the difference between predicted labels and actual labels.\n",
        "\n",
        "During training, the weights are updated iteratively using the learning rate and the computed gradients.\n",
        "\n",
        "In addition, we also maintain  **train_losses** and **test_losses** lists to store the cross-entropy loss values for the training and test sets. The final output of this implementation are the trained weight vectors as well as training loss and test loss histories."
      ],
      "metadata": {
        "id": "XTWzLaz8iuVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elA07GYgB_YU"
      },
      "outputs": [],
      "source": [
        "def softmax_regression(X, y, X_test, y_test, lr = 0.01, max_epochs=100):\n",
        "  num_samples, num_features = X.shape # extract the number of samples with which we work and define the features (x0,...,xn values)\n",
        "  num_classes = y.shape[1] # we divide the number of classes to be the number of digits\n",
        "  weights = np.zeros((num_classes, num_features)) # initialize weights to zeros\n",
        "  train_losses = [] # for visualization of train loss function\n",
        "  test_losses = [] # for visualization of test loss function\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs), desc= 'Epoch'):\n",
        "    y_pred = softmax_prob_calc(X, weights)\n",
        "    loss = cross_entropy_loss(y, y_pred)\n",
        "\n",
        "    gradient = (y_pred - y).T @ X / num_samples\n",
        "    weights-= lr * gradient # Gradient Descent\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    y_pred_test = softmax_prob_calc(X_test, weights)\n",
        "    test_loss = cross_entropy_loss(y_test, y_pred_test)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "  print(\"Training complete\")\n",
        "  return weights, train_losses, test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using these trained weights, we would be able to predict the labels for the test samples from the model, by finding the class to which each sample belongs based on the highest probability:"
      ],
      "metadata": {
        "id": "eKItIQv2nbOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9QZsmuUUg89"
      },
      "outputs": [],
      "source": [
        "def predict_softmax_regression(X, W):\n",
        "  softmax_probs = softmax_prob_calc(X, W)\n",
        "  predicted_labels = np.argmax(softmax_probs, axis=1)\n",
        "  print(\"Predictions made\")\n",
        "  return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Prediction\n",
        "Now when we have provided all the necessary implementations, we are ready to train the model on the **X_train_scaled_with_bias** set and the **y_onehot_train** set. We also input **X_test_scaled_with_bias** and **y_onehot_test** into the function in order to get the train loss and test loss results.\n",
        "\n",
        "We have chosen to use a learning rate of 0.15 because it can be considered a large enough rate to make noticeable progress per epoch but it is also not too big to mislead the model from reaching the optimal solution.\n",
        "\n",
        "We have also chosen the max_epochs to be 225, because it will allow the model to have enough time to train and correct itself on the large data set it trains on. This will also give the model the time to converge (something that might have been prevented if we would've chosen smaller max_epochs). On the other hand, we don't want to overfit our model so we didn't try to take a bigger number.\n",
        "\n",
        "The final output will be a new list of weights (named **'W'**), **train_loss** and **test_loss** histories.  \n",
        "\n"
      ],
      "metadata": {
        "id": "2o8WB6g7pBDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl0ORZhEQIWB"
      },
      "outputs": [],
      "source": [
        "W, train_losses, test_losses = softmax_regression(X_train_scaled_with_bias, y_onehot_train, X_test_scaled_with_bias, y_onehot_test, lr = 0.15, max_epochs = 225)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will make the predictions on the **X_test_scaled_with_bias** using the weights we got from the training. The output will be the predictions of the model on the test set that we have made (called y_pred)."
      ],
      "metadata": {
        "id": "Nwa-qK6jpuA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJDXFNcoUYmt"
      },
      "outputs": [],
      "source": [
        "y_pred = predict_softmax_regression(X_test_scaled_with_bias, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Graphs:\n"
      ],
      "metadata": {
        "id": "jI7E_3ZCp32Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " After applying the Softmax Regression algorithm, we will compute the **confusion matrix** for the multi-class classification problem on the **test data**. Just like in part A, we will visualize all the important details, including the train loss and the test loss functions graph:"
      ],
      "metadata": {
        "id": "PAh6C49bqbbu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1m9kDSqVGju"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate Confusion Matrix\n",
        "confusion_matrix = confusion_matrix_fn(y_test, y_pred)\n",
        "classes = confusion_matrix.shape[0]\n",
        "sensitivity = {}\n",
        "binary_confusion_matrix = {}\n",
        "\n",
        "# Accuracy and sensitivity calculation based on the definision in the Maman\n",
        "for i in tqdm(range(classes), desc=\"Generating model's Performance\"):\n",
        "  TP = confusion_matrix[i, i]\n",
        "  FP = np.sum(confusion_matrix[i, :]) - TP\n",
        "  FN = np.sum(confusion_matrix[:, i]) - TP\n",
        "  TN = np.sum(confusion_matrix) - TP - FP - FN\n",
        "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "  sensitivity[i] = TP / (TP + FN)\n",
        "  binary_confusion_matrix[i] = np.array([[TP, FP], [FN, TN]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix and the accuracy (ACC) for the multi-class classification problem on the test data:"
      ],
      "metadata": {
        "id": "UFs6v517qX_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_0C8HR0iX89"
      },
      "outputs": [],
      "source": [
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "print(f\"ACC: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrices for each digit and their computed sensitivity (TPR) for each class:"
      ],
      "metadata": {
        "id": "PdUfaadSrb_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QncAMuOkOsh"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(10), desc='Calculating Confusion Matrices and Sensitivities'):\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = binary_confusion_matrix[i], display_labels = [i,'Other digits'])\n",
        "  cm_display.plot()\n",
        "  plt.show()\n",
        "  print(f\"\\nTPR of digit {i}: {sensitivity[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAYrFRiKDYZV"
      },
      "source": [
        "### Train loss vs. Test loss graph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss function vs. Test loss function:"
      ],
      "metadata": {
        "id": "RXqV0NzgsMMm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-6ZcEEQDXOZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Softmax Regression: Training vs. Test Loss per epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Performance Evaluation:\n",
        "- Using the Softmax Regression approach, we have achieved an accuracy of **0.9717** on the given test set (97.17% accuracy)\n",
        "\n",
        "- Digit sensitivities are ranging from **0.8403** to **0.939** (digit 0 having the highest sensitivity and digit 8 and 9 having the lowest sensitivities)\n",
        "\n",
        "- Train loss and test loss functions decreased as the model continued to learn (more epochs), indicating that it's learning and generalizing well.\n",
        "\n",
        "Using these metrics we can see that the overall performance of the Softmax Regression is strong and effective. It achieves a better performance accuracy than the Perceptron, leading in an increase of 1 % (in approximation).\n",
        "\n",
        "Digit 0 appears to be the most distinguishable digit whereas digit 8 and 9 become the most challenging digits to distinguish, indicating that the algorithm may have still struggled to distinguish between their shared visual similarities with other digits (though we could argue that the algorithm managed to distinguish the digits a lot better than the Perceptron).\n",
        "\n",
        "Moreover, the model falsely indicated that the digit 4 is 9 (65 times) and the digit 5 is 3 (53 times). This also indicates that the model struggles to distinguish between these pairs of digits.\n",
        "\n",
        "In terms of the learning behaviour, as expected, the training and test losses start relatively high, and as the model trains, both the training and the test loss functions began to decrease and eventually converged. We may see that the model didn't overfit due to the fact that the train loss decreased without the test loss being increased. Therefore we can infer that the training process demonstrates suitable behavior for a well-trained Softmax Regression model.\n",
        "\n",
        "**Strengths**:\n",
        "1. **High Accuracy** - achieving 97.17% indicates the effectiveness of Softmax Regression and an improvement from the **Multi-Class Perceptron** accuracy\n",
        "\n",
        "2. **Better digit sensitivity** - The model achieved an improvement in the sensitivity ratio of the digits, meaning, the sensitivities of each digit got slightly better (in comparison with Multi-Class Perceptron).\n",
        "\n",
        "**Weaknesses**:\n",
        "1. Difficulty with complex digits - although we have reached an improvement, the algorithm still tends to struggle with digits that share some structural patterns like 8, 9 or 3.\n",
        "\n",
        "2. Softmax Regression is a more complex algorithm due to its probabilistic approach\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2NBWi2ssRjG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXIiPhcDnnYt"
      },
      "source": [
        "# Part C: Linear Regressionüìä"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression** is another approach to deal with multi-class classification problem. It is used to model the relationship between one or more input features (called **independent variables**) and an output variable (called **dependent variable**).\n",
        "\n",
        "By using this approach, we are trying to find a linear equation that attempts to predict the output from the inputs in the best way possible, doing so by minimizing the error.\n",
        "\n",
        "In order to do so, we implement our Linear Regression model by using the **Least Squares method**, which intends to minimize the sum of squared difference between the actual value 'y' and the predicted value from the model.\n",
        "\n",
        "It is important to also mention that unlike **Multi-class Perceptron** approach and the **Softmax Regression** approach, **Linear Regression** doesn't involve any iterations or epochs. This is because the **Least Squares Method** is based on solving a closed-form equation that directly computes the optimal weights, rather than using an incremental optimization approach like we have seen in the other approaches.\n",
        "\n",
        "As a result, there is no iterative process to track, and thus, we did not generate a graph showing training and test loss over epochs."
      ],
      "metadata": {
        "id": "fpv55escKOc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use Linear Regression on the digit classification problem, we will implement our model as follows:"
      ],
      "metadata": {
        "id": "DTPvATi6RDze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the **Least Squares method** to compute the weight matrix **W** (in our case 'weights'). That is achieved by solving the **Normal Equation**. It is important to mention that in our implementation we use the pseudo-inverse when we solve the normal equation, that is because X.T @ X are not always invertible (if X isn't full rank for instance):"
      ],
      "metadata": {
        "id": "Xfh5hlsvRqwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRgZhYmfnvIN"
      },
      "outputs": [],
      "source": [
        "def linear_regression(X,y):\n",
        " # The normal equation\n",
        " weights = np.linalg.pinv(X.T @ X) @ X.T @ y # Using Pseudo-inverse to compute normal equation\n",
        " print(\"Training has completed\")\n",
        " return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we want to use the computed weights to make predictions. The result is **y_pred** vector containing the predicted class labels for each input sample."
      ],
      "metadata": {
        "id": "0KuZKgzcSUpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvSlxXg8rvf1"
      },
      "outputs": [],
      "source": [
        "def predict_linear_regression(X,W):\n",
        "  Z = X @ W # logits for each class\n",
        "  y_pred = np.argmax(Z, axis = 1) # select class with the highest score for each sample\n",
        "  print(\"Predictions made\")\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Prediction"
      ],
      "metadata": {
        "id": "n24VvJLrWOpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now when we have provided all the necessary implementations, we are ready to train the model on the **X_train_scaled_with_bias** set and the **y_onehot_train** set. The output will be a new list of weights (named **'W'**).\n"
      ],
      "metadata": {
        "id": "d3cPJf6nWRS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RMl3efvuJ-w"
      },
      "outputs": [],
      "source": [
        "W = linear_regression(X_train_scaled_with_bias, y_onehot_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this new set of weights, the model will make predictions on the **X_test_scaled_with_bias** set."
      ],
      "metadata": {
        "id": "QbW8Trf3WjGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17BHYkJyvXaY"
      },
      "outputs": [],
      "source": [
        "y_pred = predict_linear_regression(X_test_scaled_with_bias, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results:"
      ],
      "metadata": {
        "id": "OBxMdbWqW3h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will evaluate the performance of the **Linear Regression** model on the test set. Just like we did in the last part, we will compute the **confusion matrix** for the multi-class classification problem on the **test data**, compute the accuracy of the model, and we will also calculate the confusion matrices for each digit, as well as the sensitivities of the digits."
      ],
      "metadata": {
        "id": "DQVy-WfoYDCR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8e2XlPFvwEc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate Confusion Matrix\n",
        "confusion_matrix = confusion_matrix_fn(y_test, y_pred)\n",
        "classes = confusion_matrix.shape[0]\n",
        "sensitivity = {}\n",
        "binary_confusion_matrix = {}\n",
        "\n",
        "# Accuracy and sensitivity calculation based on the definision in the Maman\n",
        "for i in tqdm(range(classes), desc=\"Generating model's Performance\"):\n",
        "  TP = confusion_matrix[i, i]\n",
        "  FP = np.sum(confusion_matrix[i, :]) - TP\n",
        "  FN = np.sum(confusion_matrix[:, i]) - TP\n",
        "  TN = np.sum(confusion_matrix) - TP - FP - FN\n",
        "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "  sensitivity[i] = TP / (TP + FN)\n",
        "  binary_confusion_matrix[i] = np.array([[TP, FP], [FN, TN]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The confusion matrix and the accuracy (ACC)** for the multi-class classification problem on the test data:"
      ],
      "metadata": {
        "id": "aMP5VfAmZEOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aiV3aCFvzOq"
      },
      "outputs": [],
      "source": [
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "print(f\"ACC: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrices for each digit and their computed sensitivity (TPR) for each class:"
      ],
      "metadata": {
        "id": "VKZ9B9kNZW1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(10), desc='Calculating Confusion Matrices and Sensitivities'):\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = binary_confusion_matrix[i], display_labels = [i,'Other digits'])\n",
        "  cm_display.plot()\n",
        "  plt.show()\n",
        "  print(f\"\\nTPR of digit {i}: {sensitivity[i]}\")\n"
      ],
      "metadata": {
        "id": "HkcnG9xuYatV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Evaluation and Comparison with other Approaches\n",
        "\n",
        "We will analyze the results obtained using the Linear Regression and compare them with those achieved through the **Perceptron Algorithm** and **Softmax Regression**.\n",
        "\n",
        "- Using the Linear Regression approach, we have achieved an accuracy of **0.9636** on the test set (96.36% accuracy)\n",
        "\n",
        "- Digit sensitivities are ranging from **0.8010** to **0.905** (digit 0 having the highest sensitivity and digit 4 having the lowest sensitivity)\n",
        "\n",
        "Using these metrics we can see that the overall performance of the **Linear Regression** is good. In comparison to the Perceptron Algorithm, it manages to get an improvement in the accuracy by a small margin. However, its accuracy is still lower than the **Softmax Regression**.\n",
        "\n",
        "As for the sensitivity comparisons, the highest sensitivity for **Linear Regression** is noticeably lower than the highest sensitivities achieved by the Perceptron and Softmax Regression models.\n",
        "\n",
        "(C4)\n",
        "**Strengths**:\n",
        "1. **Very Simplistic** - **Linear Regression** is easy to implement and is computationally inexpensive\n",
        "\n",
        "2. **Accuracy** - Although the accuracy isn't greater than the **Softmax Regression**'s accuracy, it is still slightly better than the **Perceptron**\n",
        "\n",
        "**Weaknesses**:\n",
        "1. **Linear Regression** assumes a linear relationship between input pixels and output labels. However, in the case of MNIST dataset, digit patterns are complex and non-linear, which in turn can make the algorithm perform poorly.\n",
        "\n",
        "2. There are other more complex models that can obtain a greater accuracy compared to **Linear Regression**.\n",
        "\n",
        "**Limitiations**:\n",
        "\n",
        "As mentioned above, **Linear Regression** assumes a linear relationship between input features and output labels but in the case of the MNIST dataset, the algorithm would only be able to reach suboptimal decision boundaries, which in turn will result in misclassifications, especially for digits with overlapping features like 4 or 9.\n",
        "\n",
        "In addition, **Linear Regression** in the implementation of **Least Squares method** doesn't benefit from iterative optimization, like the other 2 methods. Once the weights are computed through the closed-form solution, they remain fixed, even if there is a high error rate. This means **Linear Regression** cannot iteratively refine its weights to reduce errors further, unlike the Perceptron or Softmax Regression that do use iterative optimization techniques."
      ],
      "metadata": {
        "id": "nFPTS3QFZpQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "By using the implementations we have presented in each part of this notebook, we have achieved the following results:\n",
        "\n",
        "| Model                  | Accuracy  | Highest Sensitivity      | Lowest Sensitivity       |\n",
        "|------------------------|-----------|--------------------------|--------------------------|\n",
        "| Multi-Class Perceptron | 96.35%    | 0.943         | 0.785         |\n",
        "| Softmax Regression     | 97.17%    | 0.939         | 0.8403       |\n",
        "| Linear Regression      | 96.36%    | 0.905          | 0.801        |\n",
        "\n",
        "---\n",
        "\n",
        "## So what can we understand from these results?\n",
        "Based on the results we have in the table above, and the information about the implementations we have presented in each part, we can conclude that:\n",
        "\n",
        "1. **Softmax Regression** is the best-performing model in terms of accuracy and balanced sensitivity. It is ideal for tasks that require high accuracy and probabilistic insights.\n",
        "\n",
        "2. **Linear Regression** performs slightly better than **Multi-Class Perceptron** in accuracy but struggles with non-linear patterns that exist in the MNIST dataset.\n",
        "\n",
        "3. **Multi-Class Perceptron** is computationally efficient and simple to implement, making it a good choice for basic classification tasks when computational resources are limited.\n",
        "\n",
        "  - If we were to seek high accuracy and balanced performance across classes, then **Softmax Regression** would be the best choice.\n",
        "\n",
        "  - If we were to seek computational efficiency, both **Linear Regression** and **Multi-Class-Perceptron** could be suitable. However, due to the fact that **Linear Regression** is designed for regression tasks, the model can lead to suboptimal classification performance. Therefore the **Multi-class Perceptron** algorithm could be more suitable for separating\n",
        "classses in the MNIST dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "j_x9CeJez5Uw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}